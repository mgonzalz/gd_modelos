{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proceso ETL Automático: Extracción desde Azure y Carga en SQL Server Local.**\n",
    "Este notebook realiza un proceso **ETL (Extracción, Transformación y Carga)** entre dos bases de datos SQL Server: una en Azure y otra local. Se conecta a ambas fuentes, extrae datos usando archivos SQL, los transforma y los carga en tablas equivalentes dentro de una base de datos local, creando las tablas desde cero con claves primarias y foráneas definidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importación de librerías y configuración general.**\n",
    "Se cargan las librerías necesarias (incluidas en el archivo `requirements.txt`) para trabajar con bases de datos, manipulación de datos y archivos. También se desactivan ciertos mensajes de advertencia que podrían ensuciar la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conexiones a bases de datos.**\n",
    "Se definen y configuran las cadenas de conexión para acceder tanto a la **base de datos remota en Azure** como a una **base de datos local**. Las cadenas de conexión incluyen el driver ODBC, el nombre del servidor, la base de datos destino y los métodos de autenticación correspondientes.\n",
    "- **Azure:** Utilizan autenticación interactiva con Azure Active Directory.\n",
    "- **LocalHost:** Autenticación integrada de Windows (Trusted Connection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure connection string.\n",
    "AZURE_SERVER = 'uaxmathfis.database.windows.net'\n",
    "AZURE_DATABASE = 'usecases'\n",
    "AZURE_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "azure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local connection string.\n",
    "LOCAL_SERVER = 'localhost'\n",
    "LOCAL_DATABASE = 'dwh_case1'\n",
    "LOCAL_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "local_conn_str = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definición de consultas SQL.**\n",
    "Se construye un diccionario que asocia los nombres lógicos de las tablas destino con los archivos `.sql` donde se encuentran las sentencias SQL de extracción. Estas consultas se ejecutarán sobre la base de datos de Azure para obtener los datos fuente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_folder = \"../database/dwh\"\n",
    "queries = {\n",
    "    \"dim_geo\": \"load_geo.sql\",\n",
    "    \"dim_product\": \"load_product.sql\",\n",
    "    \"dim_time\": \"load_time.sql\",\n",
    "    \"dim_client\": \"load_client.sql\",\n",
    "    \"fact_sales\": \"load_fact.sql\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definición de claves primarias y foráneas.**\n",
    "Se declaran las claves primarias de cada tabla para garantizar integridad a nivel de entidad.\n",
    "Además, se especifican las claves foráneas necesarias para mantener la integridad referencial entre la tabla de hechos y las dimensiones del modelo estrella (star schema). Esta metadata será utilizada al momento de generar dinámicamente las sentencias `CREATE TABLE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary keys for each table.\n",
    "primary_keys = {\n",
    "    \"fact_sales\": [\"CODE\"],\n",
    "    \"dim_client\": [\"Customer_ID\"],\n",
    "    \"dim_geo\": [\"TIENDA_ID\"],\n",
    "    \"dim_product\": [\"Id_Producto\"],\n",
    "    \"dim_time\": [\"Fecha\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreign keys for each table.\n",
    "foreign_keys = {\n",
    "    \"fact_sales\": {\n",
    "        \"Customer_ID\": \"dim_client(Customer_ID)\",\n",
    "        \"TIENDA_ID\": \"dim_geo(TIENDA_ID)\",\n",
    "        \"Id_Producto\": \"dim_product(Id_Producto)\",\n",
    "        \"Sales_Date\": \"dim_time(Fecha)\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generación dinámica de sentencias.**\n",
    "Se implementa una función que construye sentencias `CREATE TABLE` en T-SQL a partir de la estructura del DataFrame resultante de la extracción. La función determina los tipos de datos SQL basándose en los tipos de datos de pandas/numpy, y añade restricciones de clave primaria y foráneas cuando corresponda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_sql(table_name, df):\n",
    "    # Definición de los tipos de datos SQL para cada columna del DataFrame: Por defecto tipo TEXTO.\n",
    "    col_defs = []\n",
    "    for col in df.columns:\n",
    "        if np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            col_defs.append(f'[{col}] DATE')\n",
    "        elif df[col].dtype == np.float32:\n",
    "            col_defs.append(f'[{col}] FLOAT')\n",
    "        elif df[col].dtype == np.int32:\n",
    "            col_defs.append(f'[{col}] INT')\n",
    "        else:\n",
    "            col_defs.append(f'[{col}] NVARCHAR(255)')\n",
    "\n",
    "    # Agregación clave primaria si corresponde.\n",
    "    pk = \", PRIMARY KEY (\" + \", \".join(primary_keys[table_name]) + \")\" if table_name in primary_keys else \"\"\n",
    "    # Agregación claves foráneas si corresponde.\n",
    "    fk = \"\"\n",
    "    if table_name in foreign_keys:\n",
    "        for col, ref in foreign_keys[table_name].items():\n",
    "            fk += f\", FOREIGN KEY ({col}) REFERENCES {ref}\"\n",
    "\n",
    "    return f\"CREATE TABLE {table_name} ({', '.join(col_defs)}{pk}{fk});\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función se encarga de **eliminar las tablas existentes** en la base de datos local antes de volver a crearlas durante el proceso ETL. Se eliminan en un **orden específico** que respeta las dependencias de claves foráneas, evitando errores de integridad referencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables_in_order(cursor, conn):\n",
    "    drop_order = [\"fact_sales\", \"dim_time\", \"dim_product\", \"dim_geo\", \"dim_client\"]\n",
    "    for table in drop_order:\n",
    "        # Verifica si la tabla existe en el esquema actual.\n",
    "        check_exists_query = f\"\"\"\n",
    "        IF OBJECT_ID('{table}', 'U') IS NOT NULL\n",
    "            DROP TABLE {table};\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cursor.execute(check_exists_query)\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error al intentar eliminar la tabla {table}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Proceso ETL.**\n",
    "Se establece el flujo completo de carga de datos:\n",
    "- **Extract:** Lectura y ejecución de las consultas SQL sobre Azure para obtener los datos.\n",
    "- **Transform:** Eliminación columnas duplicadas. Indentificación columnas de tipo fecha (evitando confundirlas con campos numéricos como año). Ajuste tipos de datos (int32, float32) para optimizar espacio. Relleno de valores nulos con ceros para evitar errores al insertar.\n",
    "- **Load:** Eliminación de las tablas destino en la base local si ya existen. Creación nuevamente con las restricciones adecuadas. Realización con una inserción masiva de datos utilizando `fast_executemany` para optimizar la carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexiones correctamente establecidas.\n",
      "\n",
      "Procesando: dim_geo\n",
      "   - Filas obtenidas: 12\n",
      "   - Columnas: ['TIENDA_ID', 'TIENDA_DESC', 'PROV_DESC', 'ZONA']\n",
      "   - Tabla dim_geo creada correctamente.\n",
      "   - 12 filas insertadas.\n",
      "\n",
      "Procesando: dim_product\n",
      "   - Filas obtenidas: 404\n",
      "   - Columnas: ['Id_Producto', 'Code_', 'Kw', 'TIPO_CARROCERIA', 'TRANSMISION_ID', 'Equipamiento', 'FUEL', 'Margen', 'Costetransporte', 'Margendistribuidor', 'GastosMarketing', 'Mantenimiento_medio', 'Comisión_Marca']\n",
      "   - Tabla dim_product creada correctamente.\n",
      "   - 404 filas insertadas.\n",
      "\n",
      "Procesando: dim_time\n",
      "   - Filas obtenidas: 3652\n",
      "   - Columnas: ['Fecha', 'InicioMes', 'FinMes', 'Dia', 'Diadelasemana', 'Diadelesemana_desc', 'Mes', 'Mes_desc', 'Año', 'Añomes', 'Week', 'Trimestre', 'SemanaDelMes', 'DiaDelAño', 'DiaDelTrimestre', 'Findesemana', 'Festivo', 'Laboral']\n",
      "   - Tabla dim_time creada correctamente.\n",
      "   - 3652 filas insertadas.\n",
      "\n",
      "Procesando: dim_client\n",
      "   - Filas obtenidas: 44053\n",
      "   - Columnas: ['Customer_ID', 'Edad', 'Fecha_nacimiento', 'GENERO', 'STATUS_SOCIAL', 'RENTA_MEDIA_ESTIMADA', 'ENCUESTA_ZONA_CLIENTE_VENTA', 'ENCUESTA_CLIENTE_ZONA_TALLER', 'CODIGO_POSTAL', 'poblacion', 'provincia', 'lat', 'lon', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'U2', 'Max_Mosaic_G', 'Max_Mosaic2', 'Renta_Media', 'F2', 'Mosaic_number']\n",
      "   - Tabla dim_client creada correctamente.\n",
      "   - 44053 filas insertadas.\n",
      "\n",
      "Procesando: fact_sales\n",
      "   - Filas obtenidas: 58049\n",
      "   - Columnas: ['CODE', 'Code_', 'COSTE_VENTA_NO_IMPUESTOS', 'Customer_ID', 'EN_GARANTIA', 'EXTENSION_GARANTIA', 'Fin_Garantia', 'Id_Producto', 'IMPUESTOS', 'MANTENIMIENTO_GRATUITO', 'PVP', 'Sales_Date', 'SEGURO_BATERIA_LARGO_PLAZO', 'TIENDA_ID', 'DATE_ULTIMA_REVISION', 'DIAS_DESDE_ULTIMA_REVISION', 'Km_medio_por_revision', 'km_ultima_revision', 'Revisiones', 'DIAS_DESDE_LA_ULTIMA_ENTRADA_TALLER', 'DIAS_EN_TALLER', 'QUEJA', 'MOTIVO_VENTA', 'FORMA_PAGO', 'FORMA_PAGO_GRUPO', 'Fue_Lead', 'Lead_compra', 'Logistic_date', 'Prod_date', 't_logist_days', 't_prod_date', 't_stock_dates', 'Origen', 'Car_Age', 'Margen_Eur_bruto', 'Margen_Eur', 'Coste_Total_Venta', 'ComisionTotal', 'MargenPorKw', 'Tasa_Quejas_Venta', 'Churn']\n",
      "   - Tabla fact_sales creada correctamente.\n",
      "   - 58049 filas insertadas.\n",
      "\n",
      "ETL completado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Conexión a las bases de datos.\n",
    "    conn_azure = pyodbc.connect(azure_conn_str)\n",
    "    conn_local = pyodbc.connect(local_conn_str)\n",
    "    print(\"Conexiones correctamente establecidas.\\n\")\n",
    "\n",
    "    with conn_local.cursor() as cursor:\n",
    "        drop_tables_in_order(cursor, conn_local)\n",
    "    # Procesamiento de cada tabla definida en el diccionario de Queries.\n",
    "    for table_name, file in queries.items():\n",
    "        print(f\"Procesando: {table_name}\")\n",
    "        query_path = os.path.join(query_folder, file)\n",
    "        with open(query_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sql_query = f.read()\n",
    "\n",
    "        # Ejecución de la consulta sobre la base de datos de Azure.\n",
    "        df = pd.read_sql(sql_query, conn_azure)\n",
    "\n",
    "        # Eliminación de las columnas duplicadas.\n",
    "        if df.columns.duplicated().any():\n",
    "            print(f\"Columnas duplicadas en {table_name}: {df.columns[df.columns.duplicated()].tolist()}\")\n",
    "            df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "        # Detección de las columnas tipo DATE para convertirlas adecuadamente.\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object or df[col].dtype == \"string\":\n",
    "                sample_values = df[col].astype(str).sample(min(len(df), 30), random_state=42)\n",
    "                # Saltar si parece una columna numérica (para no confundir INT con DATE).\n",
    "                if sample_values.str.isdigit().mean() > 0.8:\n",
    "                    continue\n",
    "                try:\n",
    "                    parsed = pd.to_datetime(sample_values, errors='coerce')\n",
    "                    if parsed.notna().sum() > 0.9 * len(sample_values):\n",
    "                        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                except:\n",
    "                    pass\n",
    "        # Si el DataFrame está vacío, se salta.\n",
    "        if df.empty:\n",
    "            print(f\"La tabla {table_name} no devolvió resultados.\\n\")\n",
    "            continue\n",
    "        print(f\"   - Filas obtenidas: {df.shape[0]}\")\n",
    "        print(f\"   - Columnas: {df.columns.tolist()}\")\n",
    "\n",
    "        # Limpieza de valores nulos y tipos de datos.\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].replace(r'^\\s*$', np.nan, regex=True) # Reemplazar espacios en blanco por NaN.\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                sentinel = 0\n",
    "                df[col] = df[col].fillna(sentinel)\n",
    "            elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                pass\n",
    "            else:\n",
    "                df[col] = df[col].fillna(\"N/A\")\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "\n",
    "        # Creación de la tabla en la base de datos local.\n",
    "        with conn_local.cursor() as cursor:\n",
    "            create_sql = create_table_sql(table_name, df)\n",
    "            cursor.execute(create_sql)\n",
    "            conn_local.commit()\n",
    "            print(f\"   - Tabla {table_name} creada correctamente.\")\n",
    "\n",
    "            placeholders = ', '.join(['?' for _ in df.columns])\n",
    "            insert_sql = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "            cursor.fast_executemany = True\n",
    "            cursor.executemany(insert_sql, df.values.tolist())\n",
    "            conn_local.commit()\n",
    "            print(f\"   - {df.shape[0]} filas insertadas.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'conn_azure' in locals():\n",
    "        conn_azure.close()\n",
    "    if 'conn_local' in locals():\n",
    "        conn_local.close()\n",
    "\n",
    "print(\"ETL completado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
